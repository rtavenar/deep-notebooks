{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Multi-Layer Perceptron in `keras`\n\nIn this series of lab sessions, you will use a Python library called `keras` (that is in fact embedded inside a larger library called `tensorflow`, but we will not discuss `tensorflow` in this course).\nYou should visit [`keras` webpage](https://www.tensorflow.org/guide/keras/overview) to get access to more information about this library, including a comprehensive documentation.\n\n## The `Sequential` model in `keras`\n\nThis library offers two ways to define neural network models. \nWe will start with the `Sequential` class of `keras` models.\nBelow is an example of how to define a `Sequential` model:",
   "metadata": {
    "id": "NjCKi8REqlN6",
    "cell_id": "00000-4e5bf590-09bc-42f5-a318-08994a30bfd4",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PRKkYealqiiG",
    "output_cleared": false,
    "source_hash": "b851176a",
    "execution_millis": 1216,
    "cell_id": "00001-e7a9d101-f83a-49f2-ae43-deb55009fadf",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1613342071585,
    "deepnote_cell_type": "code"
   },
   "source": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, InputLayer",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**1. Define layers, and add them one by one to the model**\n",
   "metadata": {
    "id": "1UdkphfnspbR",
    "cell_id": "00002-fddb7023-3fe5-422a-ab01-7974d23883a6",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JD54PuqWsp4Y",
    "cell_id": "00003-3e192e55-740f-4e3e-9969-016701af86a1",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e4cce2c1",
    "execution_start": 1613342072806,
    "execution_millis": 57,
    "deepnote_cell_type": "code"
   },
   "source": "input_layer = InputLayer(input_shape=(24,))\nhidden_layer1 = Dense(units=12, activation=\"relu\")\nhidden_layer2 = Dense(units=12, activation=\"sigmoid\")\n#[...]\noutput_layer = Dense(units=3, activation=\"linear\")\n\nmodel = Sequential([\n    input_layer,\n    hidden_layer1,\n    hidden_layer2,\n    # ...\n    output_layer\n])",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**2. Pick an optimization algorithm (optimizer) and a loss function to be optimized**\n\nUsual loss functions are:\n* `\"mse\"` for regression,\n* `\"categorical_crossentropy\"` for multiclass classification (when the `y` array fed to `fit` is of shape $(n, n_\\text{classes})$)\n* `\"binary_crossentropy\"` for binary classification (when the model is fed with `y` array of shape $(n, 1)$)\n\nOne can also specify additional metrics to be printed during training (correct classification rate here).",
   "metadata": {
    "id": "FTQLjyUoszDq",
    "cell_id": "00004-607dbfcd-c789-4071-98bf-f51e230e647f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A_21M9Jqs3eJ",
    "cell_id": "00005-29ab32a8-52eb-40c1-9c63-1e8b8943cbdd",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "63a93405",
    "execution_start": 1613342072870,
    "execution_millis": 9,
    "deepnote_cell_type": "code"
   },
   "source": "model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**3. Fit the model**\n\nNB: do not try to execute the following line of code: variables `X_train` and `y_train` do not exist yet!",
   "metadata": {
    "id": "ykI4bexvs5x-",
    "cell_id": "00006-1084a597-d5d5-4e2e-96ba-bf42d51d64c7",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rt89yAdWs688",
    "cell_id": "00007-8516017c-c628-4ef3-ac0f-d80ca5f74a93",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "fd94a07d",
    "execution_start": 1613342072879,
    "execution_millis": 2,
    "deepnote_cell_type": "code"
   },
   "source": "#model.fit(X_train, y_train, verbose=2, epochs=10, batch_size=200)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data pre-processing\n\nHave a look at the `prepare_mnist` and `prepare_boston` functions defined below.\n\n**Question #1.** What do these functions do? What are the shapes of returned arrays? Does the returned data correpond to classification or regression problems?",
   "metadata": {
    "id": "nHhJosslvDRY",
    "cell_id": "00008-79f8d9f3-2a42-42bb-8bcc-fc2f1f47eecd",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7l-s71YRwTRA",
    "output_cleared": false,
    "source_hash": "1615ed07",
    "execution_millis": 1636,
    "cell_id": "00009-c7c58fdf-b6f1-4f92-b82b-8ade1f4f44ca",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1613342072885,
    "deepnote_cell_type": "code"
   },
   "source": "from sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.datasets import mnist, boston_housing\nfrom tensorflow.keras.utils import to_categorical\n\ndef prepare_mnist():\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n    x_train = x_train.reshape((x_train.shape[0], -1))\n    x_test = x_test.reshape((x_test.shape[0], -1))\n    scaler = MinMaxScaler()\n    scaler.fit(x_train)\n    x_train = scaler.transform(x_train)\n    x_test = scaler.transform(x_test)\n    y_train = to_categorical(y_train)\n    y_test = to_categorical(y_test)\n    return x_train, x_test, y_train, y_test\n\n\ndef prepare_boston():\n    (x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n    scaler_x = MinMaxScaler()\n    scaler_x.fit(x_train)\n    x_train = scaler_x.transform(x_train)\n    x_test = scaler_x.transform(x_test)\n    scaler_y = MinMaxScaler()\n    scaler_y.fit(y_train.reshape((-1, 1)))\n    y_train = scaler_y.transform(y_train.reshape((-1, 1)))\n    y_test = scaler_y.transform(y_test.reshape((-1, 1)))\n    return x_train, x_test, y_train, y_test\n  \nx_train, x_test, y_train, y_test = prepare_mnist()",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CGlUHH6KFpj3",
    "output_cleared": false,
    "source_hash": "7facd6d8",
    "execution_millis": 64,
    "cell_id": "00010-3c1481af-a4d7-424b-a59e-3a2745234530",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1613342074525,
    "deepnote_cell_type": "code"
   },
   "source": "x_train, x_test, y_train, y_test = prepare_boston()",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n57344/57026 [==============================] - 0s 0us/step\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Building your first models\n\nIn the following, when fitting models, restrict the training to 10 epochs (which is not realistic, but training for more epochs takes time...)\n\n**Question #2.** Following the guidelines provided above, implement a linear regression model for the `boston` dataset that would optimize on a least squares objective using Stochastic Gradient Descent and fit your model to the corresponding training data.",
   "metadata": {
    "id": "VRA_Ec2-yIGA",
    "cell_id": "00011-1ecdf9c5-01c3-4a0d-a9c6-cfd28e6be23d",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "output_cleared": true,
    "source_hash": "b623e53d",
    "execution_millis": 1,
    "cell_id": "00013-2e740cd6-9a8b-4a74-b85f-c7a66127d510",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1613342074582,
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Question #3.** Similarly, define a logistic regression model for the `mnist` dataset and print its training accuracy during training.",
   "metadata": {
    "id": "Gqt1yH9Gzfuh",
    "cell_id": "00015-c40decce-b6c2-4f3e-af55-6dc6b2eb29cd",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00014-fa195760-a859-4672-bd64-7911ece3c703",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Question #4.** Compare performance (in terms of training accuracy, we will come back to better ways to compare models afterwards) of this logistic regression model with that of a neural network with respectively 1, 2, and 3 hidden layers of 128 neurons each.\nYou will use the `\"relu\"` activation function for hidden layers.",
   "metadata": {
    "id": "6SGA-CdS0vwW",
    "cell_id": "00018-cfad4bc9-a1de-43a4-bb43-d74840d9869d",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00016-667b31d2-ee2e-4a34-9a88-33c19a38522a",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b623e53d",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Question #5.** `keras` models offer a `count_params()` method to get the number of parameters to be learned in the model. Use this facility to get the number of parameters of your 3-hidden-layer model and build a new one-hidden-layer model with an equivalent number of parameters. Compare performance of these two models with similar number of parameters.",
   "metadata": {
    "id": "tC9EB0vs363m",
    "cell_id": "00020-a024cc10-c7b4-4021-89d9-0ed7447b2b9e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00018-d3f7b59d-095f-43d8-903e-2b92eb43619f",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b623e53d",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## A better way to compare models\n\nComparing models based on training accuracy (resp. loss) is a \"great\" way to overfit your model to the training data.\nA better way to compare models is to use hold out data (aka validation set).\n\nTo do so, `keras` allows to pass, at `fit` time, a fraction of the training data to be used as validation set. Have a look [there](https://www.tensorflow.org/guide/keras/train_and_evaluate#automatically_setting_apart_a_validation_holdout_set) for more details about how validation samples are selected.\n\n**Question #6.** Repeat model comparisons above (relying on validation scores) using 30% of training data as validation set.",
   "metadata": {
    "id": "iXxTGmUw5ppd",
    "cell_id": "00024-1bdd65e7-515d-4064-882a-54c651abfd3d",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Optimizers and learning rate\n\n**Question #7.** Change the optimizer used for your model. Use an optimizer with momentum and adaptive learning rate.",
   "metadata": {
    "id": "WehjG92yDMcN",
    "cell_id": "00026-77f877a8-a870-4358-a389-6435fd380c88",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00021-8889ee1e-48b4-4783-b8e9-fb42783d8d39",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b623e53d",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Question #8.** Using [the docs](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers), vary the learning rate of your optimizer from a very low value to a much larger one so as to show evidence of:\n* instability when the learning rate is too large;\n* slow convergence when the learning rate is too low.",
   "metadata": {
    "id": "ot8jiQHuEZxr",
    "cell_id": "00028-47912783-f891-416d-9e9d-05d57a819e7c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pZfwWRDPEa1g",
    "output_cleared": true,
    "source_hash": "b623e53d",
    "execution_millis": 38776,
    "cell_id": "00029-16623cf9-4788-418c-8696-9e61bfe444fb",
    "deepnote_to_be_reexecuted": true,
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Callbacks\n\nCallbacks are tools that, in `keras`, allow one to intervene during the training process of a model. \nCallbacks can be used to take actions (_ie._ save intermediate model, stop optimization if overfitting occurs, _etc._).\n\nA first callback one can play with is the one returned by any call to `fit` on a `keras` model.\nThis callback is an object with an `.history` attribute in the form of a Python dictionnary whose keys are the metrics recorded during training. Each of these keys links to an array containing the consecutive values of the considered quantity (one value per epoch).\n\n**Question #9.** Plot correct classification rates on both training and validation sets.",
   "metadata": {
    "id": "tyCzy_PNEqwE",
    "cell_id": "00030-f3edc3e5-83b2-4417-99c0-ce99030af441",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qA-RPKzaI4-s",
    "output_cleared": true,
    "source_hash": "b623e53d",
    "execution_millis": 67,
    "cell_id": "00031-a30d0556-649b-490f-8b57-f0ac45eb245a",
    "deepnote_to_be_reexecuted": true,
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Setting up other callbacks must be explicit. This is done by passing a list of callbacks to the `fit` method.\n\nWhen training a model is long, one can wish to record intermediate models (in case of a crash during training, or just for cases when intermediate models were performing better than the final one).\nThe [`ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) callback is designed for that purpose.\n\n**Question #10.** Set up recording of intermediate models every epoch. Save the models into a dedicated file `model.hdf5` on your Deepnote project. Only record models if validation loss is lower than for all previous models.",
   "metadata": {
    "id": "l9y1at8qI5cO",
    "cell_id": "00032-0b2aa348-08ef-45f0-8984-70863c338c77",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00027-fc377b2f-fe5b-4d0e-94b4-faa235dae4bc",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b623e53d",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Use the code below to check that a model has been saved:",
   "metadata": {
    "tags": [],
    "cell_id": "00028-e61b8442-9511-4a89-a53c-9590238e0fd2",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m0Zqsvh8Bowh",
    "cell_id": "00034-e0f2a0eb-f551-440e-94fb-2db94f18bde6",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "3b41946",
    "deepnote_cell_type": "code"
   },
   "source": "!ls -alh \"model.hdf5\"",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Regularization\n\n**Question #11.** Add an $\\ell_2$ regularization to the weights of your model and show its impact on overfitting. [These docs](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers) could help.",
   "metadata": {
    "id": "WFehub4rLrZm",
    "cell_id": "00035-1206a01d-7467-4a3e-bbdc-ee72b70366a2",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00030-dcf2874b-1d98-4a94-9f5d-a048e99081d8",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b623e53d",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Question #12.** Instead of the $\\ell_2$ regularization, set up a [drop-out](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) strategy and assess its impact on overfitting (you will turn off 10% of the neurons at each training batch).",
   "metadata": {
    "id": "vumLi62gNVhk",
    "cell_id": "00037-eeefff63-cd4e-4d79-b3ed-4f6bfb2ce154",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00031-76277bb0-9d1e-44c8-a795-8144cb710525",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b623e53d",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Question #13.** Set up an [`EarlyStopping`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) strategy such that training the model will stop in case the validation loss does not decrease for 5 consecutive epochs.",
   "metadata": {
    "id": "xbOmJKruNYyS",
    "cell_id": "00041-10a0c691-e01a-451e-807b-594ba76cc1fa",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00033-d7da52c7-7fbf-4f7d-940d-821cbe255b64",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b623e53d",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=42d3e808-a75c-4f9e-8bf1-115b96f89fc0' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "MLP.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "deepnote_notebook_id": "35aa3537-3e05-40fa-86f0-1f7ded16df52",
  "deepnote_execution_queue": [],
  "deepnote": {}
 }
}