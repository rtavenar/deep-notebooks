{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bb76fb5fef92490eabccbe7f31d7a262",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "outputs": "",
   "source": [
    "# Neural Networks for Time Series\n",
    "\n",
    "In this notebook, we'll cover topics related to learning from time series and sequential data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2d3210fdee8646a2b2aa29cf62c64e2f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "outputs": "",
   "source": [
    "## Preamble\n",
    "\n",
    "This lab session deals with the use of neural networks for time series classification and forecasting.\n",
    "Two kinds of architectures are considered here: convolutional and recurrent models. An illustration of attention-based models is provided at the end of the lab.\n",
    "\n",
    "## Time Series Classification using convolutional models (ConvNets)\n",
    "\n",
    "For a start, you will load the \"Trace\" dataset using [`tslearn` data loading tool](https://tslearn.readthedocs.io/en/latest/gen_modules/datasets/tslearn.datasets.CachedDatasets.html#tslearn.datasets.CachedDatasets.load_dataset).\n",
    "\n",
    "**Question #1.** What are the dimensions of the training data (`X_train`)? And what does each dimension correspond to (number of series, number of timestamps, number of features, ...)?\n",
    "Also, `keras` expects class information to be encoded as one-hot vectors. Use [`to_categorical`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) to format your `y` arrays accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "0cdc8bbfe0914cc9bff67edbabd76e5b",
    "deepnote_cell_type": "code",
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8b1ec9d6bb1f4698b4857a2b60328708",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "outputs": "",
   "source": [
    "**Question #2.** In the following, you will be implementing a ConvNet using keras' [`Conv1D`](https://keras.io/api/layers/convolution_layers/convolution1d/) layers.\n",
    "What value should be passed to the `data_format` parameter to match `tslearn` format? Is it the default value?\n",
    "\n",
    "**Your Answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "dceb1bf5722e40ab9cd95032758c0507",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "outputs": "",
   "source": [
    "**Question #3.** `keras` has a `Sequential` model class that allows to build models by stacking layers.\n",
    "Define a model that is made of the following layers (use ReLU activation wherever it makes sense):\n",
    "\n",
    "* a convolution layer made of 10 filters of size 3\n",
    "* a pooling layer of pool size 2\n",
    "* a flatten layer (that converts a time series of features into a flattened array that is suited to feed fully-connected layers)\n",
    "* a fully-connected layer that has as many neurons as the number of classes in the \"Trace\" problem, and an adequate activation function.\n",
    "\n",
    "Compile your model (use \"Adam\" optimizer) and fit it for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "986a1a9144e84fadb31729e71692a784",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1869,
    "execution_start": 1643388899518,
    "output_cleared": true,
    "source_hash": "4ac327ac",
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "eb7b06ceddee4e479a36bb3a9495229a",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "outputs": "",
   "source": [
    "**Question #4.** Plot the evolution of accuracy through epochs on both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "2da15a19e0cc4f379eb30b1708ce6c33",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     250
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1484,
    "execution_start": 1643388901250,
    "output_cleared": true,
    "source_hash": "abb80127",
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a6cb7b7407934e2cb1a917bed0cd64da",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "outputs": "",
   "source": [
    "**Question #5.** Ten epochs of training might not be sufficient, yet we do not know how many epochs would be necessary for a decent training. Set up early stopping (cf. [this callback](https://keras.io/api/callbacks/early_stopping/)) and see how long it takes before the model stops training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "fc03a35f7df04656bf23a6212266a4fe",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1523,
    "execution_start": 1643388902754,
    "output_cleared": true,
    "source_hash": "b57f1438",
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "23a6010e38704d70931b751dc92f6378",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "outputs": "",
   "source": [
    "## Recurrent neural nets\n",
    "\n",
    "For this new part of the lab, we will use the data generated from the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "7d66e1d72ce647f5aacf7a52058efbde",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     250
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 497,
    "execution_start": 1643388908680,
    "source_hash": "613c393b",
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_sines(n_series_per_class, length, length_padding=0):\n",
    "    t = np.linspace(0, 2 * np.pi, num=length)\n",
    "    X0 = .005 * np.random.randn(n_series_per_class, length + length_padding)\n",
    "    X0[:, :length] = np.sin(t).reshape((1, -1))\n",
    "    X0[:, length:] = np.sin(np.linspace(0, 2 * np.pi, num=length_padding))\n",
    "    \n",
    "    X1 = .005 * np.random.randn(n_series_per_class, length + length_padding)\n",
    "    X1[:, :length] = np.sin(-t).reshape((1, -1))\n",
    "    X1[:, length:] = np.sin(np.linspace(0, 2 * np.pi, num=length_padding))\n",
    "\n",
    "    dataset = np.array([X0, X1]).reshape((2 * n_series_per_class, length + length_padding, 1))\n",
    "    y = np.array([0] * n_series_per_class + [1] * n_series_per_class)\n",
    "\n",
    "    indices = np.random.permutation(2 * n_series_per_class)\n",
    "    return dataset[indices], y[indices]\n",
    "\n",
    "np.random.seed(0)\n",
    "X_train, y_train = make_sines(100, 50, length_padding=5)\n",
    "X_test, y_test = make_sines(100, 50, length_padding=5)\n",
    "\n",
    "plt.figure()\n",
    "colors = [\"r\", \"b\"]\n",
    "for ts, yi in zip(X_train, y_train):\n",
    "    plt.plot(ts.ravel(), color=colors[yi])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "99ca95a36a3f45aeade6cc9565c6db94",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "outputs": "",
   "source": [
    "**Question #7.** Implement your own recurrent layer (_cf._ formulas in the course) using the skeleton below and train a network\n",
    "made of a single recurrent unit with a 8-dimensional hidden state followed by a fully connected layer, and evaluate its classification \n",
    "performance on the dataset provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "c8aa837080c744d8bc92d0e81b8945f0",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1643388909180,
    "source_hash": "2aab574a",
    "tags": []
   },
   "outputs": "",
   "source": [
    "from keras.layers import Layer\n",
    "from keras.ops import tanh, zeros\n",
    "\n",
    "class CustomRecurrentUnit(Layer):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # List sets of parameters here\n",
    "        self.w_h = self.add_weight(\n",
    "            shape=(hidden_dim, hidden_dim), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "        self.b_h = self.add_weight(shape=(hidden_dim,), initializer=\"zeros\", trainable=True)\n",
    "        self.w_i = self.add_weight(\n",
    "            shape=(input_dim, hidden_dim), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "        self.b_i = self.add_weight(shape=(hidden_dim,), initializer=\"zeros\", trainable=True)\n",
    "    \n",
    "    def linear_hidden(self, h_t):\n",
    "        return h_t @ self.w_h + self.b_h\n",
    "    \n",
    "    def linear_input(self, x_t):\n",
    "        return x_t @ self.w_i + self.b_i\n",
    "    \n",
    "    def call(self, x):\n",
    "        n_timestamps = x.shape[1]\n",
    "        # Initialize h to [0, ..., 0]\n",
    "        # h = zeros((1, ???))\n",
    "        for t in range(n_timestamps):\n",
    "            # Update h\n",
    "            # h = tanh(???)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "22e3c8897c91438d93e01354657534f7",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "outputs": "",
   "source": [
    "**Question #8.** Implement a network made of a `CustomRecurrentUnit` followed by a fully-connected layer\n",
    "for the classification task introduced above.\n",
    "Evaluate this model both in terms of training loss and test-set accuracy (you can use the above callback to limit the amount of logging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "b040773d7c16484e8a1ea1ab4a516996",
    "deepnote_cell_type": "code",
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "592d8ae6887f4bb9a7639bea3dfa6280",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "outputs": "",
   "source": [
    "**Question #9.** Update your dataset so that it includes a final padding of 15 timestamps (_cf._ signature of the `make_sines` function)\n",
    "and see how it impacts performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "64253c69ae52450d913535407c8f0dc2",
    "deepnote_cell_type": "code",
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8c515a15d64240d6b29af74010d8d52b",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "outputs": "",
   "source": [
    "**Question #10.** Build GRU (resp. LSTM) counterparts of the RNN-based model above.\n",
    "How do they compare experimentally to the previous model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "1e0462b0f9d6459782e117794c51ef4f",
    "deepnote_cell_type": "code",
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9a3e4c2e84c041f7a18f34c6fb35cdb5",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "outputs": "",
   "source": [
    "## Recap\n",
    "\n",
    "**Question #11.** Come back to the \"Trace\" dataset used above and design a fair comparison between several convolutional and recurrent architectures to decide which one to choose for the problem at hand (feel free to play with the depth of the nets, as well as hidden representation dimensionality, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "3020b8693ec140de8384108efb3d753e",
    "deepnote_cell_type": "code",
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "## Toy seq2seq task\n",
    "\n",
    "We now turn our focus on attention-based models.\n",
    "For a start, have a look at the data produced by the following `gen_data` function.\n",
    "The task at hand is to predict the output sequence from its corresponding input one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def make_series(positions, heights, shapes, size):\n",
    "    series = np.zeros((size, ))\n",
    "    for p, h, s in zip(positions, heights, shapes):\n",
    "        if s == \"r\":\n",
    "            series[p-5:p+5] = h\n",
    "        else:\n",
    "            series[p-5:p] = np.linspace(start=0., stop=h, num=5)\n",
    "            series[p:p+5] = np.linspace(stop=0., start=h, num=5)\n",
    "    return series.reshape((-1, 1))\n",
    "    \n",
    "\n",
    "def gen_data(n_samples, noise_level=.1):\n",
    "    inputs, outputs = [], []\n",
    "    shapes = np.array([\"t\", \"r\"] * 2)\n",
    "    n_shapes = len(shapes)\n",
    "    \n",
    "    sz = 100\n",
    "    region_width = sz // n_shapes\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        np.random.shuffle(shapes)\n",
    "        index_high_t = np.random.choice(np.where(shapes == \"t\")[0], size=1)[0]\n",
    "        index_high_r = np.random.choice(np.where(shapes == \"r\")[0], size=1)[0]\n",
    "        \n",
    "        base_input_series = np.random.randn(sz, 1) * noise_level\n",
    "        heights = []\n",
    "        positions = []\n",
    "        for idx_shape in range(n_shapes):\n",
    "            pos = idx_shape * region_width + np.random.randint(low=5, high=region_width - 5)\n",
    "            height = (.5 + np.random.rand(1)[0]) * 10.\n",
    "            if idx_shape in [index_high_r, index_high_t]:\n",
    "                height += 20.\n",
    "            heights.append(height)\n",
    "            positions.append(pos)\n",
    "        input_series = np.random.randn(sz, 1) * noise_level + make_series(positions, heights, shapes, sz)\n",
    "        \n",
    "        normalized_heights = np.array(heights)\n",
    "        for s in [\"t\", \"r\"]:\n",
    "            normalized_heights[shapes == s] = normalized_heights[shapes == s].mean()\n",
    "        output_series = np.random.randn(sz, 1) * noise_level + make_series(positions, normalized_heights, shapes, sz)\n",
    "        inputs.append(input_series)\n",
    "        outputs.append(output_series)\n",
    "            \n",
    "    return np.array(inputs), np.array(outputs)\n",
    "\n",
    "np.random.seed(0)\n",
    "inputs, outputs = gen_data(1000)\n",
    "test_inputs, test_outputs = gen_data(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "**Question #12.** Visualize the first 6 input/output pairs (each in one subfigure). Can you guess how the height of output motifs is computed?\n",
    "Why would this data better suited to attention-based models than convolutional ones, for example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "**Question #13.** Check how a model made of a stack of 5 convolutional layers (use 64 filters in all layers except the last one) performs on this task. What loss function should you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "**Question #14.** Below is the definition of a multi-head self attention layer class. Use this class to replace the 3rd convolution in your model above by a **single-head self-attention layer** that would output a 64-dimensional time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": [
    "from keras.layers import MultiHeadAttention\n",
    "\n",
    "class MultiHeadSelfAttention(MultiHeadAttention):\n",
    "    def __init__(self, num_heads, key_dim, dropout=0.):\n",
    "        super().__init__(num_heads, key_dim, dropout=dropout)\n",
    "\n",
    "    def call(self, x, return_attention_scores=False):\n",
    "        return super().call(x, x, return_attention_scores=return_attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "**Question #15.** Compare both models in terms of validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "**Question #16.** Use test data for qualitative inspection of the produced outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "**Question #17.** Use the code below to visualize average attention scores for the first 3 test series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "def visualize_average_attention_scores(model, index_of_attention_layer, time_series):\n",
    "    sub_model = Sequential(model.layers[:index_of_attention_layer])\n",
    "    input_features = sub_model(time_series)\n",
    "    att_layer = model.layers[index_of_attention_layer]\n",
    "    _, weights = att_layer(input_features, return_attention_scores=True)\n",
    "\n",
    "    plt.figure(figsize=(4*len(time_series), 4))\n",
    "    for idx, ts in enumerate(time_series):\n",
    "        plt.subplot(2, len(time_series), idx + 1)\n",
    "        plt.plot(ts.ravel())\n",
    "        plt.title(\"Input series\")\n",
    "        plt.subplot(2, len(time_series), len(time_series) + idx + 1)\n",
    "        plt.plot(weights[idx, 0].numpy().mean(axis=0))\n",
    "        plt.title(\"Average attention scores\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "visualize_average_attention_scores(model, 2, test_inputs[:3])"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "25ccbef48c0041259097a232f5d6522e",
  "kernelspec": {
   "display_name": "py38_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "25f9a3951446179f6c2016b22a60b44495fe90f43bda7f3caedfe2c1a9cd31f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
