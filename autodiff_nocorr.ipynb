{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "# Basics of automatic differentiation in PyTorch\n",
    "\n",
    "In this notebook, you will go through the basic notions of automatic differentiation (aka autodiff) in PyTorch.\n",
    "\n",
    "## 1. Manual differentiation in pure Python\n",
    "\n",
    "Before starting with `pytorch` and its automatic differentiation features, let us have a look at how to do manual differentiation in Python.\n",
    "\n",
    "To do so, we will use a very basic example in 1D: let $x$ be a scalar and let $y$ be defined as:\n",
    "\n",
    "$$y = (x - .5)^2$$\n",
    "\n",
    "Our goal will be to tune $x$ in order to minimize $y$.\n",
    "\n",
    "**Question 1.1.** Define a function `f` that takes `x` as input and returns `y` as defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "In order to be able to minimize, we will use a strategy called gradient descent.\n",
    "The idea of gradient descent is to iteratively update $x$ by moving it in the opposite direction of the gradient $\\frac{\\partial y}{\\partial x}$.\n",
    "We hence need to be able to compute $\\frac{\\partial y}{\\partial x}$.\n",
    "Since we do not rely on autodiff for now, we need to provide the explicit formula for this derivative.\n",
    "\n",
    "**Question 1.2.** Define a function `grad_f` that takes `x` as input and returns $\\frac{\\partial y}{\\partial x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "The basic idea behind gradient descent is to iteratively update $x$ using the following update rule:\n",
    "\n",
    "$$x \\leftarrow x - \\eta \\frac{\\partial y}{\\partial x}$$\n",
    "\n",
    "**Question 1.3.** Define a starting value for `x` and a step size `eta` and apply gradient descent for 30 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "**Question 1.4.** Is the resulting value for `x` close to the value you would expect as a minimizer for $y = f(x)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "_YOUR ANSWER HERE_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "## 2. PyTorch and the automatic computation of gradients\n",
    "\n",
    "PyTorch is very similar to numpy in practice. One main difference is that one can ask, at any moment, for the automatic computation of gradients.\n",
    "\n",
    "To do so, if one wants to trigger the computation of $\\frac{\\partial a}{\\partial b}$ for any $b$, she should write:\n",
    "\n",
    "```python\n",
    "a.backward()\n",
    "```\n",
    "\n",
    "This will trigger the computation of the gradient of `a` with respect to any tensor that was involved in the computation of `a`.\n",
    "\n",
    "And the gradient $\\frac{\\partial a}{\\partial b}$ will be stored in `b.grad`.\n",
    "\n",
    "**Question 2.1.** Fill the code below to check what the gradient of `x` is before calling `backward()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "import torch\n",
    "\n",
    "def f(x):\n",
    "    return (x - .5) ** 2\n",
    "\n",
    "x = torch.tensor(0.125, requires_grad=True)\n",
    "y = f(x)\n",
    "\n",
    "# Fill the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "**Question 2.2.** Now, trigger the computation of gradients $\\frac{\\partial y}{\\partial x}$ and print this gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "## 3. Gradient descent in PyTorch\n",
    "\n",
    "**Question 3.1.** Try to implement the gradient descent from Section 1 in PyTorch this time. You do not need to use `grad_f` anymore in your computations.\n",
    "Each iteration should consist in:\n",
    "1. computing `y` based on the current value for `x` ;\n",
    "2. explicitly forcing gradient computations ;\n",
    "3. updating `x` (this step needs to be protected in a `with torch.no_grad():` block) ;\n",
    "4. zero-ing out gradients of `x` for future steps not to accumulate gradient computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "x = torch.tensor(0.125, requires_grad=True)\n",
    "\n",
    "stepsize = 0.1\n",
    "n_iter = 30\n",
    "\n",
    "for i in range(n_iter):\n",
    "    # Compute y and force gradient computations\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Update x\n",
    "        pass\n",
    "    # Zero-out gradients (code below is OK, leave it as it is)\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "## 4. Wrap-up: optimizing parameters of a univariate linear regression model\n",
    "\n",
    "Below is some code to generate (and visualize) the synthetic dataset you will use in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = torch.rand(100, 1)\n",
    "# w* = -3, b* = 1.5\n",
    "y = -3. * X + 1.5 + 0.4 * torch.randn(X.size())\n",
    "\n",
    "plt.scatter(X.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "You will try to fit a linear regression model to this dataset.\n",
    "\n",
    "**Question 4.1.** Given the code that generated the dataset, what should be the ideal values for $w$ and $b$ in your linear model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "_YOUR ANSWER HERE_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "**Question 4.3.** Implement a function `mse` that would take `X`, `y`, `w`, `b` as inputs and outputs the mean squared error of the linear model parametrized by `w` and `b` on the dataset $(X, y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "outputs": "",
   "source": [
    "**Question 4.2.** Implement a gradient descent loop to fit `w` and `b` that would minimize the mean squared error criterion based on the provided dataset. Use a step size of 0.1 and perform 1000 iterations of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "keep_corr"
    ]
   },
   "outputs": "",
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
